# -*- coding: utf-8 -*-
"""Taller ML Regresion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hUn5sbFmfMVB8zpP2xln9YUAQ2Q9Zfwt
"""

import pandas as pd
from scipy import stats
!pip install tableone
from tableone import TableOne
import numpy as np
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import ElasticNet
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score, log_loss
from sklearn.model_selection import  GridSearchCV
from sklearn.preprocessing import OneHotEncoder

url ="https://raw.githubusercontent.com/4GeeksAcademy/logistic-regression-project-tutorial/main/bank-marketing-campaign-data.csv"

df = pd.read_csv(url, sep=';')

df

df['y'].value_counts(normalize=True)

df['contact']

df.columns

df

def is_binary(df_, nums):
    df = df_.copy()
    variables = []
    for var in nums:
        flag = True
        unique = df_[var].unique()
        for value in unique:
            if value not in [0, 1, np.nan, 0.0, 1.0]:
                flag = False
        if flag == True:
            variables.append(var)
    return variables

def breakdown_vars(df, off_binary=False):
    """
    This function allow us categorize accodign to numerical or not
    """
    binaries = is_binary(df, df.columns)
    categorial = []
    nonormal = []
    normal = []
    for t in df.columns:
        if off_binary == False:
          if (df[t].dtypes.name=="object" or df[t].dtypes.name=='category') and  t not in binaries:
            categorial.append(t)
        else:
           if (df[t].dtypes.name=="object" or df[t].dtypes.name=='category'):
            categorial.append(t)
        if (df[t].dtypes=="int64" or df[t].dtypes=="float64") and t not in binaries:
                n,p = stats.shapiro(df[t])
                if p<0.05:
                    nonormal.append(t)
                else:
                    normal.append(t)
    if off_binary == False:
      return categorial, binaries, nonormal, normal
    else:
      return categorial, nonormal, normal

cat, nonormal ,  normal = breakdown_vars(df,  off_binary=True)

cat

print(len(df.columns))
print(len(normal) + len(nonormal) + len(cat))
mytable = TableOne(df,categorical=cat, nonnormal=nonormal, groupby='y', pval=True)

mytable.to_excel("tablauno.xlsx")

df.columns

df.drop(columns=['housing', 'loan', 'pdays', 'previous'], inplace=True)

df

df['campaign'].unique()

cat, nonormal,normal = breakdown_vars(df,  off_binary=True)

df[normal + nonormal].corr()

df[['euribor3m', 'nr.employed','emp.var.rate', 'cons.price.idx' ]].max()

df[['euribor3m', 'nr.employed','emp.var.rate', 'cons.price.idx' ]].min()

df[['euribor3m', 'nr.employed','emp.var.rate', 'cons.price.idx' ]].max() - df[['euribor3m', 'nr.employed','emp.var.rate', 'cons.price.idx' ]].min()

df[['euribor3m', 'nr.employed','emp.var.rate', 'cons.price.idx' ]].var()

df[['euribor3m', 'nr.employed','emp.var.rate', 'cons.price.idx' ]].describe()

df.drop(columns=['euribor3m','emp.var.rate', 'cons.price.idx' ], inplace=True)

cat, nonormal ,  normal = breakdown_vars(df,  off_binary=True)

df[nonormal + normal].corr()

X  = df.drop(columns = cat + ['y'])
y = df['y']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30)

# todo procesamiento de datos lo vamos a hacer aparte

y_train = np.where(y_train == 'yes', 1, 0)

def grid_lr(X_train, y_train):
    model = LogisticRegression(random_state=666, max_iter=300)
    class_weight =  [{0:0.05, 1:0.95}, {0:0.9, 1:0.01}]
    solvers = ['liblinear']
    penalty = ['l2','l1']
    c_values = [0.1,0.001, 1, 10]
    grid = dict(solver=solvers,penalty=penalty,C=c_values, class_weight= class_weight)
    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,
                           scoring='f1',error_score='raise')
    grid_result = grid_search.fit(X_train, y_train)
    return  grid_result.best_estimator_

model = grid_lr(X_train, y_train)

model

y_preds = model.predict(X_test)

y_test = np.where(y_test == 'yes', 1, 0)

print(classification_report(y_test, y_preds))

cat, nonormal,normal = breakdown_vars(df,  off_binary=True)

X  = df.drop(columns = ['y'])
y = df['y']

cat, nonormal,normal = breakdown_vars(X,  off_binary=True)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30)

X_train

def dummies_pro(X_train_, X_test_, cats):
  X_train = X_train_.copy()
  X_test = X_test_.copy()
  ohe = OneHotEncoder(drop='first',handle_unknown='ignore', sparse_output=False)
  dummies = pd.DataFrame(ohe.fit_transform(X_train[cats]), index=X_train.index)
  dummies.columns = ohe.get_feature_names_out()  #Names ohe.get_feature_names_out()-> all dummies
  X_train.drop(columns=cats, inplace=True)
  X_train = pd.concat([X_train,dummies], axis=1)
  ohe.transform(X_test[cats])
  names = ohe.get_feature_names_out()
  dummies = pd.DataFrame(ohe.transform(X_test[cats]), index=X_test.index, columns = names)
  X_test.drop(columns= cats, inplace=True)
  X_test = pd.concat([X_test,dummies], axis=1)
  return X_train, X_test

X_train_model, X_test_model = dummies_pro(X_train, X_test, cat)

X_train_model

y_train = np.where(y_train == 'yes', 1, 0)
y_test = np.where(y_test == 'yes', 1, 0)

model = grid_lr(X_train_model, y_train)

model

y_preds = model.predict(X_test_model)

print(classification_report(y_test, y_preds))